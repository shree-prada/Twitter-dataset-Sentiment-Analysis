{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzgmpf7b0EC4",
        "outputId": "2520a73d-b571-4e13-95d7-9b0132fb8384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.10/dist-packages (0.11.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aYCJiIE0oyS",
        "outputId": "cedec02c-65f8-47c9-b493-0a84b6b10d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from snscrape) (2.31.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from snscrape) (3.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->snscrape) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK3GID20051S",
        "outputId": "1127c00c-8f7a-417f-875b-4bd9be82a820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=afff21ed519581e2c84b51f3c48f197d99e8857db1ea0065707b5f7ff64f5a8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as snt\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from datetime import date, datetime\n",
        "from collections import Counter\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3aJIxC1u0Ih9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading twitter dataset file for preprocessing\n",
        "\n",
        "final=pd.read_csv(\"final.csv\")\n",
        "print(final.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wqQV6W30nF_",
        "outputId": "15b0a743-b64b-404c-882b-98fe2ffa1b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                                url  \\\n",
            "0           0  https://twitter.com/gand_oliver/status/1648233...   \n",
            "1           1  https://twitter.com/MirrorF1/status/1648233870...   \n",
            "2           2  https://twitter.com/MotorsportWeek/status/1648...   \n",
            "3           3  https://twitter.com/f1reader/status/1648233423...   \n",
            "4           4  https://twitter.com/Racingnews365C/status/1648...   \n",
            "\n",
            "                        date            id  \\\n",
            "0  2023-04-18 07:56:10+00:00  1.650000e+18   \n",
            "1  2023-04-18 07:55:52+00:00  1.650000e+18   \n",
            "2  2023-04-18 07:54:56+00:00  1.650000e+18   \n",
            "3  2023-04-18 07:54:05+00:00  1.650000e+18   \n",
            "4  2023-04-18 07:53:56+00:00  1.650000e+18   \n",
            "\n",
            "                                                user  replyCount  \\\n",
            "0  {'username': 'gand_oliver', 'id': 753267774017...           0   \n",
            "1  {'username': 'MirrorF1', 'id': 113622858, 'dis...           2   \n",
            "2  {'username': 'MotorsportWeek', 'id': 27343080,...           0   \n",
            "3  {'username': 'f1reader', 'id': 230440523, 'dis...           0   \n",
            "4  {'username': 'Racingnews365C', 'id': 135924493...           0   \n",
            "\n",
            "   retweetCount  likeCount  quoteCount lang            sourceLabel  \\\n",
            "0             0          0           0   en  Blog2Tweet Automation   \n",
            "1             2          1           0   en              TweetDeck   \n",
            "2             0          1           0   en         MotorsportWeek   \n",
            "3             0          0           0   en               F1reader   \n",
            "4             2          1           0   en                 Buffer   \n",
            "\n",
            "   retweetedTweet quotedTweet                        hashtags  viewCount  \\\n",
            "0             NaN         NaN            ['f1', 'automotive']          1   \n",
            "1             NaN         NaN                          ['F1']        176   \n",
            "2             NaN         NaN  ['Audi', 'AutoShanghai', 'F1']         65   \n",
            "3             NaN         NaN                ['F1', 'Sauber']          1   \n",
            "4             NaN         NaN      ['F1', 'F1News', 'F12023']         37   \n",
            "\n",
            "                                     renderedContent  \n",
            "0  5 Times F1 Drivers Shocked The World By Switch...  \n",
            "1  A clash between the Red Bull and Ferrari bosse...  \n",
            "2  Audi provides F1 project update at Auto Shangh...  \n",
            "3  Audi provides F1 project update at Auto Shangh...  \n",
            "4  ðŸ‘€ Audi's preparations ahead of its F1 entry in...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing \"\" with NA in retweetedTweets and quotedTweets columns\n",
        "final.fillna({\"retweetedTweet\": \"NA\", \"quotedTweet\": \"NA\"}, inplace=True)"
      ],
      "metadata": {
        "id": "Cvx9vEdo1pDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "TwtwithoutAT=[]\n",
        "sentimentScore=[]\n",
        "lemm_tokens=[]\n",
        "tfidf_matrices=[]\n",
        "for tweets in final[['renderedContent'][0]]:\n",
        "    #removing @\n",
        "    tweets = re.sub(\"@+\",\"\",tweets) #@[A-Za-z0-9]+\n",
        "    #removing hyperlinks\n",
        "    tweets = re.sub(\"(?:\\\\@|http?\\\\://|https?\\\\://|www)\\\\S+\",\"\",tweets)\n",
        "    #replacing # and _\n",
        "    tweets = tweets.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "    #removing/replacing emojis\n",
        "    tweets = emoji.replace_emoji(tweets,\"\")\n",
        "    #removing spaces from start and end\n",
        "    tweets = re.sub(r'\\n','', tweets).strip()\n",
        "    # Replace Unicode characters in the 'tweets' variable\n",
        "    tweets = tweets.replace(\"\\u2019\", \"'\")\n",
        "    tweets = tweets.replace(\"\\u2018\", \"'\")\n",
        "    tweets = tweets.replace(\"\\u2026\", \"...\")\n",
        "    tweets = tweets.replace(\"\\u2013\", \"-\")\n",
        "    tweets = tweets.replace(\"\\u201c\", \"\\\"\")\n",
        "    tweets = tweets.replace(\"\\u201d\", \"\\\"\")\n",
        "\n",
        "    text = tweets\n",
        "\n",
        "  # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "  # Stop words removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    # TF-IDF\n",
        "    documents = [\" \".join(lemmatized_tokens)]  # Creating a single document for simplicity\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "    #Appending the final output to the lists\n",
        "    lemm_tokens.append(lemmatized_tokens)\n",
        "    tfidf_matrices.append(tfidf_matrix)\n",
        "    TwtwithoutAT.append(text)\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentimentScore.append(analyzer.polarity_scores(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LKCywjr1uU3",
        "outputId": "ac6ee768-ba26-45d1-c332-aa29eedf70bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#determining Sentiment values in categorical form\n",
        "\n",
        "SentimentVal=[]\n",
        "\n",
        "for i in sentimentScore:\n",
        "    if i[\"compound\"]>0:\n",
        "        SentimentVal.append(\"Positive\")\n",
        "    elif i[\"compound\"]<0:\n",
        "        SentimentVal.append(\"Negative\")\n",
        "    else:\n",
        "        SentimentVal.append(\"Neutral\")"
      ],
      "metadata": {
        "id": "H2iBcuQ-7537"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final['RefinedContent']=TwtwithoutAT\n",
        "final['lemmatized_tokens']=lemm_tokens\n",
        "final['tfidf_matrix']=tfidf_matrices\n",
        "final['SentimentValue']=SentimentVal"
      ],
      "metadata": {
        "id": "Iv2WmfX277eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_csv(\"Twt_Refined_Final.csv\")"
      ],
      "metadata": {
        "id": "-LY8os088M0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into the DataFrame\n",
        "final = pd.read_csv(\"Twt_Refined_Final.csv\")\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numerical_cols = final.select_dtypes(include=['float64']).columns.tolist()\n",
        "categorical_cols = final.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"Numerical Columns:\")\n",
        "print(numerical_cols)\n",
        "\n",
        "print(\"\\nCategorical Columns:\")\n",
        "print(categorical_cols)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VO4vS8r9hs3",
        "outputId": "3f8e44d6-7368-4d0b-808c-e2b27bf98f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical Columns:\n",
            "['id', 'retweetedTweet']\n",
            "\n",
            "Categorical Columns:\n",
            "['url', 'date', 'user', 'lang', 'sourceLabel', 'quotedTweet', 'hashtags', 'renderedContent', 'RefinedContent', 'lemmatized_tokens', 'tfidf_matrix', 'SentimentValue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Detect and handle outliers using IQR\n",
        "for col in numerical_cols:\n",
        "    Q1 = final[col].quantile(0.25)\n",
        "    Q3 = final[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    final[col] = final[col].apply(lambda x: max(lower_bound, min(upper_bound, x)))\n",
        "\n",
        "# Save the modified DataFrame\n",
        "final.to_csv(\"Twt_Refined_Final_NoOutliers.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "OAJZT8qq9loT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = final[numerical_cols].corr()\n",
        "\n",
        "# Save the correlation matrix to a CSV file\n",
        "correlation_matrix.to_csv(\"Correlation_Matrix.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Q9o5KmGWFHct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the categorical columns you want to one-hot encode\n",
        "categorical_cols = ['SentimentValue']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encoded_df = pd.get_dummies(final, columns=categorical_cols)\n",
        "\n",
        "# Save the encoded DataFrame to a CSV file\n",
        "encoded_df.to_csv(\"Encoded_Twt_Refined_Final.csv\", index=False)"
      ],
      "metadata": {
        "id": "vJQyvelKF6An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into the DataFrame\n",
        "data = pd.read_csv(\"Encoded_Twt_Refined_Final.csv\")\n",
        "\n",
        "# Specify the columns you want to normalize (numerical columns)\n",
        "numerical_cols = ['SentimentValue_Positive', 'SentimentValue_Neutral', 'SentimentValue_Negative']\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max normalization to the specified numerical columns\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Save the normalized DataFrame to a new CSV file\n",
        "data.to_csv(\"Normalized_Encoded_Twt_Refined_Final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Sb0jhr6fH5BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, the typical order is: Normalization -> Scaling -> Dimensionality Reduction (if needed).\n",
        "\n",
        "However, keep in mind that the specific order might vary based on your data, the algorithms you're using, and the goals of your analysis. It's a good practice to experiment with different preprocessing orders and evaluate their impact on your specific task to determine the most suitable approach."
      ],
      "metadata": {
        "id": "Mex8jgPCI604"
      }
    }
  ]
}