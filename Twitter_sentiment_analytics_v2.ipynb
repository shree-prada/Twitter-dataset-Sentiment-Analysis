{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzgmpf7b0EC4",
        "outputId": "2520a73d-b571-4e13-95d7-9b0132fb8384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.10/dist-packages (0.11.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snscrape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aYCJiIE0oyS",
        "outputId": "cedec02c-65f8-47c9-b493-0a84b6b10d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from snscrape) (2.31.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from snscrape) (3.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->snscrape) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK3GID20051S",
        "outputId": "1127c00c-8f7a-417f-875b-4bd9be82a820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=afff21ed519581e2c84b51f3c48f197d99e8857db1ea0065707b5f7ff64f5a8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snscrape.modules.twitter as snt\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from datetime import date, datetime\n",
        "from collections import Counter\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3aJIxC1u0Ih9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading twitter dataset file for preprocessing\n",
        "\n",
        "final=pd.read_csv(\"final.csv\")\n",
        "print(final.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wqQV6W30nF_",
        "outputId": "15b0a743-b64b-404c-882b-98fe2ffa1b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                                url  \\\n",
            "0           0  https://twitter.com/gand_oliver/status/1648233...   \n",
            "1           1  https://twitter.com/MirrorF1/status/1648233870...   \n",
            "2           2  https://twitter.com/MotorsportWeek/status/1648...   \n",
            "3           3  https://twitter.com/f1reader/status/1648233423...   \n",
            "4           4  https://twitter.com/Racingnews365C/status/1648...   \n",
            "\n",
            "                        date            id  \\\n",
            "0  2023-04-18 07:56:10+00:00  1.650000e+18   \n",
            "1  2023-04-18 07:55:52+00:00  1.650000e+18   \n",
            "2  2023-04-18 07:54:56+00:00  1.650000e+18   \n",
            "3  2023-04-18 07:54:05+00:00  1.650000e+18   \n",
            "4  2023-04-18 07:53:56+00:00  1.650000e+18   \n",
            "\n",
            "                                                user  replyCount  \\\n",
            "0  {'username': 'gand_oliver', 'id': 753267774017...           0   \n",
            "1  {'username': 'MirrorF1', 'id': 113622858, 'dis...           2   \n",
            "2  {'username': 'MotorsportWeek', 'id': 27343080,...           0   \n",
            "3  {'username': 'f1reader', 'id': 230440523, 'dis...           0   \n",
            "4  {'username': 'Racingnews365C', 'id': 135924493...           0   \n",
            "\n",
            "   retweetCount  likeCount  quoteCount lang            sourceLabel  \\\n",
            "0             0          0           0   en  Blog2Tweet Automation   \n",
            "1             2          1           0   en              TweetDeck   \n",
            "2             0          1           0   en         MotorsportWeek   \n",
            "3             0          0           0   en               F1reader   \n",
            "4             2          1           0   en                 Buffer   \n",
            "\n",
            "   retweetedTweet quotedTweet                        hashtags  viewCount  \\\n",
            "0             NaN         NaN            ['f1', 'automotive']          1   \n",
            "1             NaN         NaN                          ['F1']        176   \n",
            "2             NaN         NaN  ['Audi', 'AutoShanghai', 'F1']         65   \n",
            "3             NaN         NaN                ['F1', 'Sauber']          1   \n",
            "4             NaN         NaN      ['F1', 'F1News', 'F12023']         37   \n",
            "\n",
            "                                     renderedContent  \n",
            "0  5 Times F1 Drivers Shocked The World By Switch...  \n",
            "1  A clash between the Red Bull and Ferrari bosse...  \n",
            "2  Audi provides F1 project update at Auto Shangh...  \n",
            "3  Audi provides F1 project update at Auto Shangh...  \n",
            "4  👀 Audi's preparations ahead of its F1 entry in...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing \"\" with NA in retweetedTweets and quotedTweets columns\n",
        "final.fillna({\"retweetedTweet\": \"NA\", \"quotedTweet\": \"NA\"}, inplace=True)"
      ],
      "metadata": {
        "id": "Cvx9vEdo1pDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "TwtwithoutAT=[]\n",
        "sentimentScore=[]\n",
        "lemm_tokens=[]\n",
        "tfidf_matrices=[]\n",
        "for tweets in final[['renderedContent'][0]]:\n",
        "    #removing @\n",
        "    tweets = re.sub(\"@+\",\"\",tweets) #@[A-Za-z0-9]+\n",
        "    #removing hyperlinks\n",
        "    tweets = re.sub(\"(?:\\\\@|http?\\\\://|https?\\\\://|www)\\\\S+\",\"\",tweets)\n",
        "    #replacing # and _\n",
        "    tweets = tweets.replace(\"#\", \"\").replace(\"_\", \" \")\n",
        "    #removing/replacing emojis\n",
        "    tweets = emoji.replace_emoji(tweets,\"\")\n",
        "    #removing spaces from start and end\n",
        "    tweets = re.sub(r'\\n','', tweets).strip()\n",
        "    # Replace Unicode characters in the 'tweets' variable\n",
        "    tweets = tweets.replace(\"\\u2019\", \"'\")\n",
        "    tweets = tweets.replace(\"\\u2018\", \"'\")\n",
        "    tweets = tweets.replace(\"\\u2026\", \"...\")\n",
        "    tweets = tweets.replace(\"\\u2013\", \"-\")\n",
        "    tweets = tweets.replace(\"\\u201c\", \"\\\"\")\n",
        "    tweets = tweets.replace(\"\\u201d\", \"\\\"\")\n",
        "\n",
        "    text = tweets\n",
        "\n",
        "  # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "  # Stop words removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "    # TF-IDF\n",
        "    documents = [\" \".join(lemmatized_tokens)]  # Creating a single document for simplicity\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "    #Appending the final output to the lists\n",
        "    lemm_tokens.append(lemmatized_tokens)\n",
        "    tfidf_matrices.append(tfidf_matrix)\n",
        "    TwtwithoutAT.append(text)\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentimentScore.append(analyzer.polarity_scores(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LKCywjr1uU3",
        "outputId": "ac6ee768-ba26-45d1-c332-aa29eedf70bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#determining Sentiment values in categorical form\n",
        "\n",
        "SentimentVal=[]\n",
        "\n",
        "for i in sentimentScore:\n",
        "    if i[\"compound\"]>0:\n",
        "        SentimentVal.append(\"Positive\")\n",
        "    elif i[\"compound\"]<0:\n",
        "        SentimentVal.append(\"Negative\")\n",
        "    else:\n",
        "        SentimentVal.append(\"Neutral\")"
      ],
      "metadata": {
        "id": "H2iBcuQ-7537"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final['RefinedContent']=TwtwithoutAT\n",
        "final['lemmatized_tokens']=lemm_tokens\n",
        "final['tfidf_matrix']=tfidf_matrices\n",
        "final['SentimentValue']=SentimentVal"
      ],
      "metadata": {
        "id": "Iv2WmfX277eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.to_csv(\"Twt_Refined_Final.csv\")"
      ],
      "metadata": {
        "id": "-LY8os088M0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into the DataFrame\n",
        "final = pd.read_csv(\"Twt_Refined_Final.csv\")\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numerical_cols = final.select_dtypes(include=['float64']).columns.tolist()\n",
        "categorical_cols = final.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"Numerical Columns:\")\n",
        "print(numerical_cols)\n",
        "\n",
        "print(\"\\nCategorical Columns:\")\n",
        "print(categorical_cols)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VO4vS8r9hs3",
        "outputId": "3f8e44d6-7368-4d0b-808c-e2b27bf98f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical Columns:\n",
            "['id', 'retweetedTweet']\n",
            "\n",
            "Categorical Columns:\n",
            "['url', 'date', 'user', 'lang', 'sourceLabel', 'quotedTweet', 'hashtags', 'renderedContent', 'RefinedContent', 'lemmatized_tokens', 'tfidf_matrix', 'SentimentValue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Detect and handle outliers using IQR\n",
        "for col in numerical_cols:\n",
        "    Q1 = final[col].quantile(0.25)\n",
        "    Q3 = final[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    final[col] = final[col].apply(lambda x: max(lower_bound, min(upper_bound, x)))\n",
        "\n",
        "# Save the modified DataFrame\n",
        "final.to_csv(\"Twt_Refined_Final_NoOutliers.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "OAJZT8qq9loT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = final[numerical_cols].corr()\n",
        "\n",
        "# Save the correlation matrix to a CSV file\n",
        "correlation_matrix.to_csv(\"Correlation_Matrix.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Q9o5KmGWFHct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the categorical columns you want to one-hot encode\n",
        "categorical_cols = ['SentimentValue']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encoded_df = pd.get_dummies(final, columns=categorical_cols)\n",
        "\n",
        "# Save the encoded DataFrame to a CSV file\n",
        "encoded_df.to_csv(\"Encoded_Twt_Refined_Final.csv\", index=False)"
      ],
      "metadata": {
        "id": "vJQyvelKF6An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into the DataFrame\n",
        "data = pd.read_csv(\"Encoded_Twt_Refined_Final.csv\")\n",
        "\n",
        "# Specify the columns you want to normalize (numerical columns)\n",
        "numerical_cols = ['SentimentValue_Positive', 'SentimentValue_Neutral', 'SentimentValue_Negative']\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max normalization to the specified numerical columns\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "# Save the normalized DataFrame to a new CSV file\n",
        "data.to_csv(\"Normalized_Encoded_Twt_Refined_Final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Sb0jhr6fH5BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, the typical order is: Normalization -> Scaling -> Dimensionality Reduction (if needed).\n",
        "\n",
        "However, keep in mind that the specific order might vary based on your data, the algorithms you're using, and the goals of your analysis. It's a good practice to experiment with different preprocessing orders and evaluate their impact on your specific task to determine the most suitable approach."
      ],
      "metadata": {
        "id": "Mex8jgPCI604"
      }
    }
  ]
}