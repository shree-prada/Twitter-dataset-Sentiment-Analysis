{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as snt\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from datetime import date, datetime\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                                url   \n",
      "0           0  https://twitter.com/gand_oliver/status/1648233...  \\\n",
      "1           1  https://twitter.com/MirrorF1/status/1648233870...   \n",
      "2           2  https://twitter.com/MotorsportWeek/status/1648...   \n",
      "3           3  https://twitter.com/f1reader/status/1648233423...   \n",
      "4           4  https://twitter.com/Racingnews365C/status/1648...   \n",
      "\n",
      "                        date            id   \n",
      "0  2023-04-18 07:56:10+00:00  1.650000e+18  \\\n",
      "1  2023-04-18 07:55:52+00:00  1.650000e+18   \n",
      "2  2023-04-18 07:54:56+00:00  1.650000e+18   \n",
      "3  2023-04-18 07:54:05+00:00  1.650000e+18   \n",
      "4  2023-04-18 07:53:56+00:00  1.650000e+18   \n",
      "\n",
      "                                                user  replyCount   \n",
      "0  {'username': 'gand_oliver', 'id': 753267774017...           0  \\\n",
      "1  {'username': 'MirrorF1', 'id': 113622858, 'dis...           2   \n",
      "2  {'username': 'MotorsportWeek', 'id': 27343080,...           0   \n",
      "3  {'username': 'f1reader', 'id': 230440523, 'dis...           0   \n",
      "4  {'username': 'Racingnews365C', 'id': 135924493...           0   \n",
      "\n",
      "   retweetCount  likeCount  quoteCount lang            sourceLabel   \n",
      "0             0          0           0   en  Blog2Tweet Automation  \\\n",
      "1             2          1           0   en              TweetDeck   \n",
      "2             0          1           0   en         MotorsportWeek   \n",
      "3             0          0           0   en               F1reader   \n",
      "4             2          1           0   en                 Buffer   \n",
      "\n",
      "   retweetedTweet quotedTweet                        hashtags  viewCount   \n",
      "0             NaN         NaN            ['f1', 'automotive']          1  \\\n",
      "1             NaN         NaN                          ['F1']        176   \n",
      "2             NaN         NaN  ['Audi', 'AutoShanghai', 'F1']         65   \n",
      "3             NaN         NaN                ['F1', 'Sauber']          1   \n",
      "4             NaN         NaN      ['F1', 'F1News', 'F12023']         37   \n",
      "\n",
      "                                     renderedContent  \n",
      "0  5 Times F1 Drivers Shocked The World By Switch...  \n",
      "1  A clash between the Red Bull and Ferrari bosse...  \n",
      "2  Audi provides F1 project update at Auto Shangh...  \n",
      "3  Audi provides F1 project update at Auto Shangh...  \n",
      "4  ðŸ‘€ Audi's preparations ahead of its F1 entry in...  \n"
     ]
    }
   ],
   "source": [
    "#Reading twitter dataset file for preprocessing\n",
    "\n",
    "final=pd.read_csv(\"final.csv\")\n",
    "print(final.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing \"\" with NA in retweetedTweets and quotedTweets columns\n",
    "final.fillna({\"retweetedTweet\": \"NA\", \"quotedTweet\": \"NA\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization, stop words removal, stemming, lemmatization, calculation of tfidf starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "TwtwithoutAT=[]\n",
    "sentimentScore=[]\n",
    "lemm_tokens=[]\n",
    "tfidf_matrices=[]\n",
    "for tweets in final[['renderedContent'][0]]:\n",
    "    #removing @\n",
    "    tweets = re.sub(\"@+\",\"\",tweets) #@[A-Za-z0-9]+\n",
    "    #removing hyperlinks\n",
    "    tweets = re.sub(\"(?:\\\\@|http?\\\\://|https?\\\\://|www)\\\\S+\",\"\",tweets)\n",
    "    #replacing # and _\n",
    "    tweets = tweets.replace(\"#\", \"\").replace(\"_\", \" \")\n",
    "    #removing/replacing emojis\n",
    "    tweets = emoji.replace_emoji(tweets,\"\")\n",
    "    #removing spaces from start and end\n",
    "    tweets = re.sub(r'\\n','', tweets).strip()\n",
    "    # Replace Unicode characters in the 'tweets' variable\n",
    "    tweets = tweets.replace(\"\\u2019\", \"'\")\n",
    "    tweets = tweets.replace(\"\\u2018\", \"'\")\n",
    "    tweets = tweets.replace(\"\\u2026\", \"...\")\n",
    "    tweets = tweets.replace(\"\\u2013\", \"-\")\n",
    "    tweets = tweets.replace(\"\\u201c\", \"\\\"\")\n",
    "    tweets = tweets.replace(\"\\u201d\", \"\\\"\")\n",
    "\n",
    "    text = tweets\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Stop words removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "\n",
    "    # TF-IDF\n",
    "    documents = [\" \".join(lemmatized_tokens)]  # Creating a single document for simplicity\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    #Appending the final output to the lists\n",
    "    lemm_tokens.append(lemmatized_tokens)\n",
    "    tfidf_matrices.append(tfidf_matrix)\n",
    "    TwtwithoutAT.append(text)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentimentScore.append(analyzer.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining Sentiment values in categorical form\n",
    "\n",
    "SentimentVal=[]\n",
    "\n",
    "for i in sentimentScore:\n",
    "    if i[\"compound\"]>0:\n",
    "        SentimentVal.append(\"Positive\")\n",
    "    elif i[\"compound\"]<0:\n",
    "        SentimentVal.append(\"Negative\")\n",
    "    else:\n",
    "        SentimentVal.append(\"Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['RefinedContent']=TwtwithoutAT\n",
    "final['lemmatized_tokens']=lemm_tokens\n",
    "final['tfidf_matrix']=tfidf_matrices\n",
    "final['SentimentValue']=SentimentVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"Twt_Refined_Final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
